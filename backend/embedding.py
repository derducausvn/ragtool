"""
embedding.py
--------------------------
Handles embedding of documents and vector store operations using OpenAI + PGVector.
Stores embeddings persistently in PostgreSQL for production use on Render.
"""

import os
import logging
import re
from typing import List
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector

# Load environment variables
load_dotenv()

# Constants & Configurations
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")  # PostgreSQL connection string
COLLECTION_NAME = "rag_embeddings"  # Table name for embeddings
MAX_CHARS = 800
CHUNK_OVERLAP = 100
BATCH_SIZE = 100

# Caches for singleton-like reuse
_embeddings = None
_text_splitter = None

def get_embeddings():
    """Initialize and return a cached OpenAI embedding instance."""
    global _embeddings
    if _embeddings is None:
        _embeddings = OpenAIEmbeddings(
            api_key=OPENAI_API_KEY,
            model="text-embedding-3-large"  
        )
    return _embeddings

def get_text_splitter():
    """Initialize and return a consistent text splitter."""
    global _text_splitter
    if _text_splitter is None:
        _text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=MAX_CHARS,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
    return _text_splitter

def chunk_documents(docs: List[Document]) -> List[Document]:
    """
    Split large documents into smaller chunks to improve embedding granularity.
    Metadata is preserved and enriched with chunk info.
    Also sanitizes content to remove problematic characters.
    """
    splitter = get_text_splitter()
    chunked_docs = []

    for doc in docs:
        # Sanitize content - remove NUL bytes and other problematic characters
        sanitized_content = doc.page_content.replace('\x00', '').replace('\ufeff', '')
        # Remove other control characters except newlines and tabs
        sanitized_content = ''.join(char for char in sanitized_content 
                                  if ord(char) >= 32 or char in '\n\t\r')
        doc.page_content = sanitized_content
        
        if len(doc.page_content) <= MAX_CHARS:
            chunked_docs.append(doc)
        else:
            chunks = splitter.split_documents([doc])
            for i, chunk in enumerate(chunks):
                # Sanitize chunk content too
                chunk.page_content = chunk.page_content.replace('\x00', '').replace('\ufeff', '')
                chunk.page_content = ''.join(char for char in chunk.page_content 
                                           if ord(char) >= 32 or char in '\n\t\r')
                
                chunk.metadata = {
                    **doc.metadata,
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                    'original_doc_length': len(doc.page_content)
                }
            chunked_docs.extend(chunks)
            logging.info(f"Split document '{doc.metadata.get('source', '')}' into {len(chunks)} chunks")

    return chunked_docs

def embed_documents(docs: List[Document], batch_size: int = BATCH_SIZE) -> int:
    """
    Embed documents and persist them in PostgreSQL using PGVector.
    Creates or adds to the existing vector store.
    """
    if not docs:
        logging.info("No documents to embed.")
        return 0

    chunked_docs = chunk_documents(docs)
    if not chunked_docs:
        logging.warning("No documents to embed after chunking.")
        return 0

    embeddings = get_embeddings()
    embedded_count = 0

    try:
        # Create PGVector instance
        db = PGVector(
            embeddings=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        
        # Add documents in batches with better error handling
        total_batches = (len(chunked_docs) + batch_size - 1) // batch_size
        failed_batches = []
        
        for i in range(0, len(chunked_docs), batch_size):
            batch_num = i // batch_size + 1
            batch = chunked_docs[i:i + batch_size]
            
            try:
                # Extra sanitization for this batch
                for doc in batch:
                    # Additional cleaning to prevent NUL byte errors
                    doc.page_content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', doc.page_content)
                    # Ensure metadata values are JSON-serializable
                    for key, value in doc.metadata.items():
                        if isinstance(value, str):
                            doc.metadata[key] = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', value)
                
                db.add_documents(batch)
                embedded_count += len(batch)
                logging.info(f"âœ… Embedded batch {batch_num}/{total_batches}: {len(batch)} documents")
                
            except Exception as e:
                failed_batches.append(batch_num)
                logging.error(f"âŒ Failed to embed batch {batch_num}/{total_batches}: {e}")
                # Try to process documents individually in this batch
                individual_successes = 0
                for doc in batch:
                    try:
                        db.add_documents([doc])
                        individual_successes += 1
                        embedded_count += 1
                    except Exception as doc_error:
                        logging.warning(f"   Skipped document from batch {batch_num}: {doc_error}")
                
                if individual_successes > 0:
                    logging.info(f"   Recovered {individual_successes}/{len(batch)} documents individually")
        
        # Summary logging
        if failed_batches:
            logging.warning(f"âš ï¸  Failed batches: {failed_batches} (but some documents may have been recovered)")
        logging.info(f"ðŸŽ¯ Total embedded: {embedded_count}/{len(chunked_docs)} documents")
        
        return embedded_count

    except Exception as e:
        logging.error(f"âŒ Embedding process failed completely: {e}")
        return 0

def search_documents(query: str, k: int = 5) -> List[Document]:
    """
    Perform similarity search on the embedded documents using PGVector.
    """
    try:
        embeddings = get_embeddings()
        db = PGVector(
            embeddings=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        docs = db.similarity_search(query, k=k)

        logging.info(f"ðŸ” Query: '{query}' â†’ {len(docs)} matches")
        return docs

    except Exception as e:
        logging.error(f"âŒ Error during vector search: {e}")
        return []

def get_vector_store_stats() -> str:
    """
    Return basic stats about the PGVector store.
    """
    try:
        embeddings = get_embeddings()
        db = PGVector(
            embeddings=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        # Note: PGVector doesn't have a direct equivalent to ntotal
        # We'll do a simple query to check if the store is accessible
        test_docs = db.similarity_search("test", k=1)
        return f"Vector store is accessible (collection: {COLLECTION_NAME})"
    except Exception as e:
        return f"Failed to access vector store: {e}"

def get_sync_stats_live():
    """
    Get real-time KPI stats from PGVector store.
    Returns number of embedded chunks and website-based entries.
    """
    try:
        # First try to get exact counts from PostgreSQL
        exact_total, exact_websites = get_exact_document_count()
        
        if exact_total is not None:
            # We got exact counts from PostgreSQL
            return {
                "documents": exact_total,
                "websites": exact_websites,
                "lastSync": "Recently synced" if exact_total > 0 else None,
                "syncProgress": 100
            }
        
        # Fallback to similarity search estimation
        embeddings = get_embeddings()
        db = PGVector(
            embeddings=embeddings,
            connection=DATABASE_URL,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        
        # Get a larger sample to estimate total documents
        # We'll use a high k value and search for a common term
        sample_docs = db.similarity_search("the", k=5000)  # Get up to 5000 docs
        total_chunks = len(sample_docs)
        
        # If we got exactly 5000, there might be more - try to get a better estimate
        if total_chunks == 5000:
            # Try a few more searches to get a better sample
            additional_samples = [
                db.similarity_search("F24", k=1000),
                db.similarity_search("question", k=1000),
                db.similarity_search("document", k=1000),
            ]
            all_seen_ids = set()
            for docs in [sample_docs] + additional_samples:
                for doc in docs:
                    # Use content hash as unique identifier
                    doc_id = hash(doc.page_content[:100])
                    all_seen_ids.add(doc_id)
            total_chunks = len(all_seen_ids)
        
        # Count website-based chunks from our sample
        website_chunks = sum(
            1 for doc in sample_docs
            if doc.metadata.get("source", "").startswith("http")
        )
        
        # Get last sync time (approximate - when we last had docs)
        last_sync = None
        if sample_docs:
            # Use the collection name as a proxy for when data was last added
            last_sync = "Recently synced"

        logging.info(f"ðŸ“Š Estimated stats: {total_chunks} total chunks, {website_chunks} from websites")

        return {
            "documents": total_chunks,
            "websites": website_chunks,
            "lastSync": last_sync,
            "syncProgress": 100
        }

    except Exception as e:
        logging.error(f"Failed to get sync stats: {e}")
        return {"documents": 0, "websites": 0, "lastSync": None, "syncProgress": 0, "error": str(e)}

def get_exact_document_count():
    """
    Get exact document count by querying PostgreSQL directly.
    This is more accurate than similarity search sampling.
    """
    try:
        import psycopg2
        from urllib.parse import urlparse
        
        # Parse DATABASE_URL to get connection parameters
        url = urlparse(DATABASE_URL)
        
        conn = psycopg2.connect(
            host=url.hostname,
            port=url.port,
            database=url.path[1:],  # Remove leading slash
            user=url.username,
            password=url.password
        )
        
        cursor = conn.cursor()
        
        # Check if the tables exist
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_name IN ('langchain_pg_collection', 'langchain_pg_embedding')
        """)
        
        existing_tables = [row[0] for row in cursor.fetchall()]
        
        if 'langchain_pg_collection' not in existing_tables or 'langchain_pg_embedding' not in existing_tables:
            # Tables don't exist yet
            logging.info("ðŸ“Š PostgreSQL tables don't exist yet")
            cursor.close()
            conn.close()
            return 0, 0
        
        # Get collection UUID
        cursor.execute("SELECT uuid FROM langchain_pg_collection WHERE name = %s", (COLLECTION_NAME,))
        collection_result = cursor.fetchone()
        
        if not collection_result:
            # Collection doesn't exist yet
            logging.info(f"ðŸ“Š Collection '{COLLECTION_NAME}' doesn't exist yet in PostgreSQL")
            cursor.close()
            conn.close()
            return 0, 0
            
        collection_id = collection_result[0]
        
        # Count documents
        try:
            cursor.execute("SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = %s", (collection_id,))
            result = cursor.fetchone()
            total_count = result[0] if result and len(result) > 0 else 0
        except Exception as count_error:
            logging.warning(f"Could not count total documents: {count_error}")
            total_count = 0
        
        # Count website chunks
        try:
            if total_count > 0:
                cursor.execute("""
                    SELECT COUNT(*) FROM langchain_pg_embedding 
                    WHERE collection_id = %s 
                    AND cmetadata ->> 'source' LIKE 'http%'
                """, (collection_id,))
                result = cursor.fetchone()
                website_count = result[0] if result and len(result) > 0 else 0
                
                # Debug: Let's see what sources we actually have
                cursor.execute("""
                    SELECT DISTINCT cmetadata ->> 'source' FROM langchain_pg_embedding 
                    WHERE collection_id = %s 
                    AND cmetadata ->> 'source' IS NOT NULL
                    LIMIT 10
                """, (collection_id,))
                sources_sample = cursor.fetchall()
                logging.info(f"ðŸ“Š Sample sources in DB: {[s[0] for s in sources_sample]}")
                
            else:
                website_count = 0
        except Exception as web_count_error:
            logging.debug(f"Website count query failed: {web_count_error}")
            website_count = 0
        
        cursor.close()
        conn.close()
        
        logging.info(f"ðŸ“Š Exact counts: {total_count} total chunks, {website_count} from websites")
        
        return total_count, website_count
        
    except Exception as e:
        logging.warning(f"Could not get exact count from PostgreSQL: {e}")
        return None, None

# --- END OF FILE ---
